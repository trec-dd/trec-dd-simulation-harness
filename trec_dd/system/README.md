# Example Recommender Systems

This directory holds example recommender systems to demonstrate usage
of the simulation harness. Right now, the only example system is
random_system.py.

# Executing the Random Recommender System

## Requirements

To run the random recommender system, you must have truth data stored
in a file-backed kvlayer store. The truth data must be stored as
dossier.label Label objects, using a LabelStore. We have a utility
script 'trec\_dd/harness/generate\_labels\_from\_runfile.py', which should
help you convert your truth data into the required format, if all you
have is a runfile generated by human assessors.

You also need a "topic sequence" file that describes *how* you want to
evaluate your system. The "topic sequence" file specifies which topics
to explore, and how many batches to request for each topic. The file
should be in yaml, and simply be a mapping from topic\_id to an
integer representing how many batches to execute for that
topic\_id. You can find an example "topic sequence" file at
trec_dd/system/example\_topic\_seq.py.

## Running the System

You can run the random recommender system in the simulation harness by
calling

    python random_system.py path/to/topic_sequence.yaml path/to/truth_data.kvl path/to/runfile_out.runfile

After this command executes, you should find the resulting system
runfile at the path you specified in the command. The runfile summarizes
the responses the random system gave to the harness, as well as the harness's
thoughts on those responses. This runfile captures everything one needs to
know in order to give a system a score.

## Scoring the System

To score your runfile, you may use the trec_dd/scorer/run.py script.

    python trec_dd/scorer/run.py path/to/runfile path/to/truthdata.kvl --scorer scorer1 scorer2 scorer3 ...

This command scans through your runfile and uses the list of provided scorers to score it. The
available scorers are as follows:

 * reciprocal\_rank\_at\_recall
 * precision\_at\_recall
 * modified\_precision\_at\_recall
 * average\_err\_arithmetic
 * average\_err\_harmonic
 * average\_err\_arithmetic\_binary
 * average\_err\_harmonic\_binary

Please see trec_dd/scorer/README.md for more information.
